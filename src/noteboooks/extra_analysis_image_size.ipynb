{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import  Dataset \n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from typing import Tuple, Dict \n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    experiment_id           = 'LG38'\n",
    "    model_name              = 'UNet'\n",
    "    train_bs                = 8\n",
    "    valid_bs                = 8\n",
    "    crop_size               = (64 , 64)\n",
    "    num_crops               = 1\n",
    "    epochs                  = 3\n",
    "    lr                      = 0.0001\n",
    "    data_train_test_split   = 0.9\n",
    "    data_train_val_split    = 0.8\n",
    "    device                  = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    mask_train              = False\n",
    "    cancerous_train         = True\n",
    "    hard_to_classify_train  = False\n",
    "    inflammatory_train      = False\n",
    "    stroma_train            = False\n",
    "    \n",
    "enabled_masks = {\n",
    "    \"cancerous\": CFG.cancerous_train,\n",
    "    \"hard_to_classify\": CFG.hard_to_classify_train,\n",
    "    \"inflammatory\": CFG.inflammatory_train,\n",
    "    \"stroma\": CFG.stroma_train\n",
    "}\n",
    "class_names = [name for name, enabled in enabled_masks.items() if enabled]\n",
    "num_masks = sum([CFG.mask_train, CFG.cancerous_train, CFG.hard_to_classify_train, CFG.inflammatory_train, CFG.stroma_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_kaggle() -> bool:\n",
    "    return os.path.exists('/kaggle/')\n",
    "\n",
    "def is_notebook() -> bool:\n",
    "\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True  \n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  \n",
    "        else:\n",
    "            return False  \n",
    "    except NameError:\n",
    "        return False  \n",
    "\n",
    "if is_kaggle():\n",
    "    dataset_path = '/kaggle/input/'\n",
    "    reports_dir = '/kaggle/working/'\n",
    "    models_dir = '/kaggle/working/'\n",
    "else:\n",
    "    if is_notebook():\n",
    "        os.makedirs(os.path.join(os.path.dirname(os.getcwd()), 'reports'), exist_ok=True) \n",
    "        dataset_path = os.path.join(os.path.dirname(os.getcwd()), 'data')        \n",
    "        reports_dir = os.path.join(os.path.dirname(os.getcwd()), 'reports')\n",
    "        models_dir = os.path.join(os.path.dirname(os.getcwd()), 'models')\n",
    "        \n",
    "    else:\n",
    "        os.makedirs(os.path.join(os.path.dirname(os.getcwd()), 'capstrone_group5','src', 'reports'), exist_ok=True) \n",
    "        dataset_path = os.path.join(os.path.dirname(os.getcwd()), 'capstrone_group5','src', 'data')\n",
    "        reports_dir = os.path.join(os.path.dirname(os.getcwd()), 'capstone_group5', 'src', 'reports')\n",
    "        models_dir = os.path.join(os.path.dirname(os.getcwd()), 'capstone_group5', 'src', 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device index: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Check your PyTorch installation and GPU drivers.\")\n",
    "allocated_memory = torch.cuda.memory_allocated()\n",
    "GPU_used = round(allocated_memory / 1024**3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of active masks is {num_masks}.\\n\")\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "print(f\"Dataset path: {reports_dir}\")\n",
    "print(f\"Dataset path: {models_dir}\\n\")\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\\n\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"GPU Memory Allocated: {allocated_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CREATING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalImageDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, mask_config: Dict[str, bool]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory with images and masks.\n",
    "            mask_config (dict): Dictionary where keys are mask names and values are True/False.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.mask_config = mask_config\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.mask_paths = {key: [] for key in mask_config.keys() if mask_config[key]}\n",
    "\n",
    "        for subdir, dirs, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\"ROI.png\"):\n",
    "                    image_path = os.path.join(subdir, file)\n",
    "                    mask_base_path = image_path.replace(\"ROI.png\", \"\")\n",
    "\n",
    "                    for mask_type in self.mask_config.keys():\n",
    "                        if self.mask_config[mask_type]:\n",
    "                            mask_path = mask_base_path + mask_type.upper() + \".png\"\n",
    "                            if os.path.exists(mask_path):\n",
    "                                self.mask_paths[mask_type].append(mask_path)\n",
    "                            else:\n",
    "                                raise FileNotFoundError(f\"Mask path {mask_path} does not exist.\")\n",
    "                    \n",
    "                    self.image_paths.append(image_path)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Image.Image, Dict[str, Image.Image]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index for data retrieval.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Image.Image, Dict[str, Image.Image]]: The original image and its corresponding masks.\n",
    "        \"\"\"\n",
    "        # Load image\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Load masks\n",
    "        masks = {}\n",
    "        for mask_type in self.mask_config.keys():\n",
    "            if self.mask_config[mask_type]:\n",
    "                mask_path = self.mask_paths[mask_type][idx]\n",
    "                mask = Image.open(mask_path).convert(\"L\")\n",
    "                masks[mask_type] = mask\n",
    "\n",
    "        # Return the original image and its masks\n",
    "        return image, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    #transforms.Resize(CFG.img_size),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mask_config = {\n",
    "    \"cancerous\": CFG.cancerous_train,\n",
    "    \"hard_to_classify\": CFG.hard_to_classify_train,\n",
    "    \"inflammatory\": CFG.inflammatory_train,\n",
    "    \"stroma\": CFG.stroma_train,\n",
    "    \"mask\": CFG.mask_train\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original = OriginalImageDataset(root_dir=dataset_path, mask_config=mask_config)\n",
    "image, masks = dataset_original[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the photo size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = []\n",
    "heights = []\n",
    "\n",
    "for i in range(len(dataset_original)):\n",
    "    image, _ = dataset_original[i]  # Assuming dataset[i] returns (image, masks)\n",
    "    \n",
    "    if not isinstance(image, torch.Tensor):\n",
    "        image = transforms.ToTensor()(image)\n",
    "    \n",
    "    _, h, w = image.shape  # Shape is typically (C, H, W)\n",
    "    widths.append(w)\n",
    "    heights.append(h)\n",
    "\n",
    "\n",
    "# Scatter plot of image widths vs heights\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(widths, heights, alpha=0.5)\n",
    "plt.title(\"Scatter Plot of Image Widths vs Heights\")\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Height\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Histogram of image widths\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(widths, bins=30, alpha=0.7, color='blue')\n",
    "plt.title(\"Histogram of Image Widths\")\n",
    "plt.xlabel(\"Width\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Histogram of image heights\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(heights, bins=30, alpha=0.7, color='green')\n",
    "plt.title(\"Histogram of Image Heights\")\n",
    "plt.xlabel(\"Height\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Median widths: {np.median(widths)}\")\n",
    "print(f\"Median heights: {np.median(heights)}\")\n",
    "print(f\"Mean widths: {np.mean(widths)}\")\n",
    "print(f\"Mean heights: {np.mean(heights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iqr(data):\n",
    "    q75, q25 = np.percentile(data, [75, 25])  \n",
    "    return q75 - q25\n",
    "\n",
    "widths_iqr = calculate_iqr(widths)\n",
    "heights_iqr = calculate_iqr(heights)\n",
    "\n",
    "# Output results\n",
    "print(f\"Widths IQR: {widths_iqr}\")\n",
    "print(f\"Heights IQR: {heights_iqr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSIONS**\n",
    " \n",
    "1. Median width is 931 and median heights is 913, with a mean value being slightly higher. \n",
    "2. IQR: the middle 50% of the widths (between the 25th and 75th percentile) is spread accross a range of 738 pixels. This indicates rather high level of variation in the widths data, there are likely some higher outliers in the dataset.\n",
    "3. IQR: the middle 50% of the heights data is spread acrross 656 pixels, this indicates as well rather high level of variation in the heights.\n",
    "4. For both width and height the distribution of the size is left-skewed, majority of the images does not exceeds 1000 px.\n",
    "5. By including the images with too much size  (e.x.64x64), the picture is not sufficiently covered covered by the kernel convolution function.\n",
    "6. The minimum size necessary for this project is 256x256 as a limit for the available computation power, however the images still offer possibility to extend the input image size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
